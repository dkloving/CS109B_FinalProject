{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109B Milestone 2\n",
    "\n",
    "### Dor Baruch, Michaela Kane, David Loving, & Brandon Walker, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_He et al. “Deep Residual Learning for Image Recognition”_\n",
    "\n",
    "\n",
    "He et al tried to use a residual learning framework to more easily train a deep neural network. In other words, they reformulated layers of the network as learning residual functions with reference to layer inputs instead of learning unreferenced functions. Although the use of deep neural networks is known to improve image classification, there remains the issue of the degradation problem: as network depth increases, network accuracy degrades, but not necessarily due to overfitting. This indicates taht not all systems are equally easy to optimize, thus calling for the use of the deep residual learning framework which explicitly lets stacked layers fit a desired underlying mapping (i.e. a residual mapping). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Ronneberger et al. \"U-Net: Convolutional Networks for Biomedical Image Segmentation_\n",
    "\n",
    "In order to improve network training such that it could be used for more specific localizing labels in biomedial image processing, Ronneberger et al used both data augmentation and a fully convolutional network. The network employed a contracting path to capture the context in an image, while symmetric expanding was used to enable precise localization. These techniques allowed for the creation of a \"fully convolutional network\" that would work with few training image and yield more precise segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dai et al \"Towards Diverse and Natural Image Descriptions via a Conditional GAN_\n",
    "\n",
    "Because neural networks are trained to maximize the likelihood of training samples, caption outputs for networks are often rigid and lack in variability, since training on training data encourages outputs that exactly match said data as opposed to other, perfectly reasonable descriptions. In order to counter this, Dai et al try to improve the naturalness and diversity of caption outputs using a new framework based on Conditional Generative Adversarial Networks (CGAN). This network produces descriptions conditioned on images with an evaluator to assess the outputs. It makes use of the Policy Gradient (a strategy that stems from Reinforcement Learning) to help the generator receive feedback even as it operates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
