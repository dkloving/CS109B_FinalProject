\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{graphicx}
%\usepackage{lmodern}
\usepackage[T1]{fontenc}
%\usepackage{textcomp}
\usepackage{pxfonts}
\usepackage{enumerate,verbatim,cite}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}
%\usepackage{../../tex/pythonhighlight} %https://github.com/olivierverdier/python-latex-highlighting

\usepackage{fancyhdr}
\pagestyle{fancy}
%\addtolength{\headheight}{\baselineskip}
\addtolength{\footskip}{\baselineskip}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0.4pt}
\fancyhf{}
\fancyfoot[L]{\textit{Last Modified: \today}}
\fancyfoot[C]{\thepage}


\usepackage[pdftex,bookmarks,hyperfigures,colorlinks
						,urlcolor=blue
						,citecolor=blue
						,linkcolor=blue
						,pdfstartview=FitH]{hyperref}


%\usepackage{epsf}
% \topmargin -0.5in \setlength{\textwidth}{6.in}
% \setlength{\textheight}{8.5in}
%\setlength{\evensidemargin}{0.25in}
% \setlength{\oddsidemargin}{0.25in}
\renewcommand{\r}{{\bf r}}
\newcommand{\dery}{\frac{dx}{dt}}
\renewcommand{\k}{{\bf k}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
%\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage{hyperref}

\newcommand{\myTitleBox}{
\noindent\makebox[\linewidth][c]{%
  %
    \parbox{\paperwidth}{%
      \hspace*{\dimexpr\hoffset+\oddsidemargin+1in\relax}%
      \begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
      {\large\textbf{\courseTitleS}\courseTitle\hfill}\vspace{2mm}\\
%      {\large\textbf{\topicsCoveredS}\topicsCovered\hfill}\vspace{2mm}\\
      {\large\courseInstructors\hfill}\vspace{2mm}\\
%      \secAuthor\hfill\sectionTimesV\\
%      \authorContact\hfill\sectionTime\\
      \end{minipage}
    %
  }%
}
}


\newcommand{\courseTitleS}{CS109B/STAT121B/APCOMP209a/CSCI109B}
\newcommand{\courseTitle}{ Advanced Topics in Data Science}
\newcommand{\courseInstructors}{\textbf{Instructors:} Mark Glickman, Pavlos Protopapas}
%\newcommand{\sectionTimesV}{Section Times}
%\newcommand{\sectionTime}{ Wed 3-4pm \& Wed 5:30-6:30 \& Thurs 2:30-3:30}
%\newcommand{\topicsCoveredS}{Advanced Section 6:}
%\newcommand{\topicsCovered}{ Topics in Supervised Classification}
%\newcommand{\authorContact}{nhoernle@g.harvard.edu}
%\newcommand{\secAuthor}{Nick Hoernle}
\newcommand{\emptyS}{ }

%\title{\textbf{CS 109A Final Project} \\ \bigskip \large{Urban Crime Prediction}}
\date{}

\begin{document}
%\myTitleBox

\noindent {\small{\sc{CS 109B/Stat 121B/AC 209B/CSCI 109B: Final Project} \hfill \\ \small{\sc{Glickman, Protopapas}} \hfill \\ 
\small{\sc{Dor Baruch, Michaela Kane, David Loving, \& Brandon Walker}}}
\begin{center}
\section*{Cancer Diagnosis in Medical Imaging}
\end{center}

%\maketitle

\subsection*{Problem Statement}

In the treatment and prevention of cancer, early detection plays a crucial and often life-saving role. One of the most common methods of early cancer detection is the CT (computer tomography) scan, which is used to detect anomalies in the form of pre-cancerous or cancerous nodules in body tissue. Human radiologists then study these scans and assess the severity and danger of any irregularities they observe.

However, the detection of nodules and their correct diagnosis is extremely challenging, as pre-cancerous nodules are often small, and even when they are visible, they often look like surrounding benign tissue formations [\ref{bib:baker}]. In an attempt to assist radiologists, recent years have seen the development and use of neural networks to help classify CT scans and other medical images for the sake of more accurate, early diagnosis and cancer prevention.

These networks are not infallible themselves, unfortunately, and there are instances in which humans place too much confidence in artificial intelligence without enough critical thought [\ref{bib:finale}]. As a result, there is an increasing demand for networks with a degree of explainability: if researchers and doctors can understand why a network output what it did, or what effects certain input changes have on the output, it can be easier for them to accept or reject the network's diagnosis and make faster, more accurate assessments. 

For this project, we attempted to address this issue of explainability in a two-step process: first, we trained a U-Net on CT Scans from LUng Nodule Analysis (LUNA) in order to create a network that could ascertain the presence of and locate lesions in lung CT Scans. We then built off of pre-trained architectures in Keras to create and train a Bayesian model using Mammographies from the Digital Database for Screening Mammography (DDSM). We use a Bayesian model here in order to enhance the explainability of our model by classifying lesion types not with point estimates, but with probability distributions. 

\subsection*{Data Recources}

\begin{enumerate}
\item \textbf{\href{https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI}{LUng Nodule Analysis (LUNA) CT Scans}}\\
Contains 888 3-dimensional CT scans of patients' lungs (601 patients with diagnosed cancer, 287 without diagnosed cancer). Each 3D CT Scan consists of XXX 2D ``slices'' of 512 by 512 pixels.

\begin{itemize}
\item[-] feature set 1
\item[-] feature set 2
\item[-] feature set 3
\end{itemize}

\item \textbf{\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3041807}{Digital Database for Screening Mammography (DDSM) Mammographies}}
Contains over 10,000 mammographies (4,505 with lesions, 6,206 without lesions). Also includes a csv file denoting lesion types, shapes, and pathologies (i.e. whether or not a lesion is malignant, benign, benign without callback, or unknown). 

\begin{itemize}
\item[-] feature set 1
\item[-] feature set 2
\item[-] feature set 3
\end{itemize}
\end{enumerate}

\subsection*{Literature Review}

In preparation for this project, we conducted research on both image classification neural networks, as well as on the importance and history of explainability in neural networks.

**TODO: explain model research most relevant to our model**

A recurring theme we found during our research was that of the importance of explainability in models. According to Doshi-Velez et al., by exposing the logic behind a decision, errors can be avoided through correction, and a greater trust for the model can be built [\ref{bib:finale}]. In other words, if researchers and doctors understand the ``thought process'' of a model,  that can allow them to use the network's output to inform their own decisions rather than dictating diagnoses. 

\subsection*{Modeling Approach}

\subsubsection*{U-Net}
While many popular images segmentation algorithms generate bounding boxes for object localization, in the biomedical field, this is often not good enough. While bounding boxes provide the general location of an object, it still falls short of offering the object's exact location. Going beyond a bounding box to pixel level classification can save an enormous amount of time waiting for experts to annotate biomedical images. Additionally, many approaches require thousands of images to train. Generating this amount of training data is typically not possible in most biomedical settings. Therefore, developing an approach that can be trained on a relatively small amount of training data would be hugely beneficial.  

The U-Net Architecture [\ref{bib:ronneberger}] addresses each of the concerns above. The U-Net localizes objects within an image at the pixel level and relies heavily on data augmentation for training, drastically reducing the amount of training data necessary to achieve good performance. 

\subsection*{Results and Interpretation}

\subsection*{Conclusion and Future Work}
- train these two networks on the same data set

\subsection*{References}

\begin{enumerate}
\item\label{bib:baker} Baker, Darren, et al. \textit{Predicting Lung Cancer Incidence from CT Imagery}. Stanford University, 2017.

\item\label{bib:finale} Doshi-Velez, Finale, et al. ``Accountability of AI Under the Law: The Role of Explanation.'' \textit{SSRN Electronic Journal}, 2017, doi:10.2139/ssrn.3064761.

\item\label{ref:gal} Gal, Yarin, and Zoubin Ghahramani. \textit{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}. University of Cambridge, 2016.

\item\label{bib:he} He, Kaiming, et al. ``Deep Residual Learning for Image Recognition.'' \textit{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016, doi:10.1109/cvpr.2016.90.

\item\label{bib:ronneberger} Ronneberger, Olaf, et al. ``U-Net Convolutional Networks for Biomedical Image Segmentation.'' \textit{Informatik Aktuell Bildverarbeitung FÃ¼r Die Medizin}, 2017, doi:10.1007/978-3-662-54345.
\end{enumerate}


\end{document}